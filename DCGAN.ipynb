{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN2 (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcSLjD22QM1a"
      },
      "source": [
        "from numpy import expand_dims\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy import vstack\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from keras.datasets.cifar10 import load_data\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Dropout\n",
        "from tensorflow.keras import datasets\n",
        "from matplotlib import pyplot\n",
        "import tensorflow as tf\n",
        "import imageio\n",
        "import PIL\n",
        "import glob\n",
        "from IPython import display"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5fTloTfRDR4"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ-LaJ7_sqPz"
      },
      "source": [
        "# Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahCZmx9UtZMH"
      },
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import zipfile\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers\n",
        "import pandas as pd \n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7c-4vRfspPn"
      },
      "source": [
        "TMP_DATA_DIR = \"dataset/tmp\"\n",
        "TMP_LABELS_DIR = os.path.join(TMP_DATA_DIR, \"GTSRB/Final_Test\")\n",
        "\n",
        "TRAINING_DATA_DIR = \"dataset/training\"\n",
        "VALIDATION_DATA_DIR = \"dataset/validation\""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x18lZmMJszQV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d5b96c4-350c-4ad4-9a7a-2aa91f2958b2"
      },
      "source": [
        "#Fetch images deom GTSRB website\n",
        "#Images for training\n",
        "!curl -LOC - https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Training_Images.zip\n",
        "#Images for validation    \n",
        "!curl -LOC - https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_Images.zip\n",
        "#Labels for validation\n",
        "!curl -LOC - https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_GT.zip"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "** Resuming transfer from byte position 276294756\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "curl: (33) HTTP server doesn't seem to support byte ranges. Cannot resume.\n",
            "** Resuming transfer from byte position 88978620\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "curl: (33) HTTP server doesn't seem to support byte ranges. Cannot resume.\n",
            "** Resuming transfer from byte position 99620\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "curl: (33) HTTP server doesn't seem to support byte ranges. Cannot resume.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgTS34gJs10s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa19b9b0-7b15-4e9d-f8bb-44564809d3df"
      },
      "source": [
        "%%time\n",
        "to_unpack = [\n",
        "    (\"GTSRB_Final_Training_Images.zip\", TMP_DATA_DIR),\n",
        "    (\"GTSRB_Final_Test_Images.zip\", TMP_DATA_DIR),\n",
        "    (\"GTSRB_Final_Test_GT.zip\", TMP_LABELS_DIR)\n",
        "]\n",
        " \n",
        "for file, directory in to_unpack:\n",
        "    print(\"Unzipping {} to {}...\".format(file, directory))\n",
        "    with zipfile.ZipFile(file,\"r\") as zip_ref:\n",
        "        zip_ref.extractall(directory)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unzipping GTSRB_Final_Training_Images.zip to dataset/tmp...\n",
            "Unzipping GTSRB_Final_Test_Images.zip to dataset/tmp...\n",
            "Unzipping GTSRB_Final_Test_GT.zip to dataset/tmp/GTSRB/Final_Test...\n",
            "CPU times: user 7.37 s, sys: 2.2 s, total: 9.57 s\n",
            "Wall time: 9.64 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-iahiyRs6SK"
      },
      "source": [
        "# Collect all PPM files and their labels\n",
        "\n",
        "tmp_train_data_dir = os.path.join(TMP_DATA_DIR, \"GTSRB/Final_Training/Images\")\n",
        "\n",
        "# Get all subdirectories of data_dir. Each represents a label.\n",
        "directories = [d for d in os.listdir(tmp_train_data_dir) \n",
        "               if os.path.isdir(os.path.join(tmp_train_data_dir, d))]\n",
        "# Loop through the label directories and collect the data in two lists, labels and images.\n",
        "ppm_files_train = []\n",
        "ppm_labels_train = []\n",
        "for class_directory in directories:\n",
        "    label_dir = os.path.join(tmp_train_data_dir, class_directory)\n",
        "    file_names = [os.path.join(label_dir, f) \n",
        "                  for f in os.listdir(label_dir) if f.endswith(\".ppm\")]\n",
        "    # For each label, load it's images and add them to the images list.\n",
        "    # And add the label number (i.e. directory name) to the labels list.\n",
        "    for image_file in file_names:\n",
        "        ppm_files_train.append(image_file)\n",
        "        ppm_labels_train.append(class_directory)\n",
        "        \n",
        "# Let's have it sorted for better debugging.\n",
        "ppm_files_train.sort()\n",
        "ppm_labels_train.sort()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg9PGIfStC_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "215cd3a4-4cf6-436e-c9aa-80a301050e52"
      },
      "source": [
        "%%time\n",
        "# Convert\n",
        "# from: dataset/Final_Training/Images/00000/00000_00000.ppm\n",
        "# to:   dataset/training/00000/00000_00000.jpg\n",
        "\n",
        "for ppm_file, label in zip(ppm_files_train, ppm_labels_train):\n",
        "    image = Image.open(ppm_file)\n",
        "    directory = os.path.join(TRAINING_DATA_DIR, label)\n",
        "    image_filename = \"{}.jpg\".format(os.path.splitext(os.path.basename(ppm_file))[0])\n",
        "\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "    \n",
        "    image.save(os.path.join(directory, image_filename))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 10.4 s, sys: 2.03 s, total: 12.5 s\n",
            "Wall time: 12.5 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uk4YQUXMuj2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a447ebc-d865-496f-b441-1c836eb80560"
      },
      "source": [
        "# Print all categories with example image for each of them.\n",
        "preprocessed_training_dirs = [d for d in os.listdir(TRAINING_DATA_DIR) \n",
        "               if os.path.isdir(os.path.join(TRAINING_DATA_DIR, d))]\n",
        "preprocessed_training_dirs.sort()\n",
        "\n",
        "train_images = []\n",
        "for training_dir in preprocessed_training_dirs:\n",
        "    train_images.append(os.path.join(TRAINING_DATA_DIR, training_dir, \"00000_00000.jpg\"))\n",
        "\n",
        "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/256)\n",
        "image_data = image_generator.flow_from_directory(str(TRAINING_DATA_DIR), target_size=(32, 32))\n",
        "\n",
        "for image_batch, label_batch in image_data:\n",
        "  print(\"Image batch shape: \", image_batch.shape)\n",
        "  print(\"Label batch shape: \", label_batch.shape)\n",
        "  break"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 39209 images belonging to 43 classes.\n",
            "Image batch shape:  (32, 32, 32, 3)\n",
            "Label batch shape:  (32, 43)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZwINXFpvxLk"
      },
      "source": [
        "#Load testing set\n",
        "tmp_validation_data_dir = os.path.join(TMP_DATA_DIR, \"GTSRB/Final_Test/Images\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8Nzv1gOv3Do",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b6e363d-d952-4d9b-cefc-535829b96fc8"
      },
      "source": [
        "%%time\n",
        "\n",
        "tmp_validation_data_files = [f for f in os.listdir(tmp_validation_data_dir) if f.endswith(\".ppm\")]\n",
        "test_images = []\n",
        "\n",
        "#export as JPGs\n",
        "for ppm_file in tmp_validation_data_files:\n",
        "    image_dir = os.path.join(tmp_validation_data_dir, ppm_file) \n",
        "    image = Image.open(image_dir)\n",
        "    directory = VALIDATION_DATA_DIR\n",
        "    image_filename = \"{}.jpg\".format(os.path.splitext(os.path.basename(ppm_file))[0])\n",
        "\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "        \n",
        "    final_image = os.path.join(directory, image_filename)\n",
        "    image.save(final_image)\n",
        "\n",
        "    test_images.append(final_image)\n",
        "    test_images.sort()\n",
        "    \n",
        "print(\"Validation images count:\", len(test_images))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation images count: 12630\n",
            "CPU times: user 5.52 s, sys: 762 ms, total: 6.29 s\n",
            "Wall time: 6.29 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcnikEfC4Z_p"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW5ErfCS5PDn"
      },
      "source": [
        "from skimage import color, exposure, transform\n",
        "from skimage import io\n",
        "import os\n",
        "import glob\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "def preprocess_img(img):\n",
        "    # Histogram normalization in v channel\n",
        "    hsv = color.rgb2hsv(img)\n",
        "    hsv[:, :, 2] = exposure.equalize_hist(hsv[:, :, 2])\n",
        "    img = color.hsv2rgb(hsv)\n",
        "\n",
        "    # central square crop\n",
        "    min_side = min(img.shape[:-1])\n",
        "    centre = img.shape[0] // 2, img.shape[1] // 2\n",
        "    img = img[centre[0] - min_side // 2:centre[0] + min_side // 2,\n",
        "              centre[1] - min_side // 2:centre[1] + min_side // 2,\n",
        "              :]\n",
        "\n",
        "    # rescale to standard size\n",
        "    img = transform.resize(img, (32, 32))\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def get_class(img_path):\n",
        "    return int(img_path.split(os.sep)[-2])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdJyOJ2C4XTP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b20f514-73ef-48ba-a1e4-553c640c045a"
      },
      "source": [
        "root_dir = 'dataset' + os.sep + 'training' + os.sep\n",
        "\n",
        "imgs = []\n",
        "labels = []\n",
        "\n",
        "all_img_paths = glob.glob(os.path.join(root_dir, '*/*.jpg'))\n",
        "\n",
        "np.random.shuffle(all_img_paths)\n",
        "for img_path in all_img_paths:\n",
        "    img = preprocess_img(mpimg.imread(img_path))\n",
        "    label = get_class(img_path)\n",
        "    imgs.append(img)\n",
        "    labels.append(label)\n",
        "\n",
        "X = np.array(imgs, dtype='float32')\n",
        "# Make one hot targets\n",
        "Y = np.eye(43, dtype='uint8')[labels]\n",
        "\n",
        "print(len(X))\n",
        "\n",
        "# Split data into training and validation (test) set\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39209\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "oHcvfWFjdzQm",
        "outputId": "1f37290a-963c-4e67-c4b3-96ec900a6a7a"
      },
      "source": [
        "print(len(train_images))\n",
        "image = train_images[1000]\n",
        "\n",
        "pyplot.imshow(image[:, :, :])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31367\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fd36fab23c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGxbzu29y9DC"
      },
      "source": [
        "# Defining discriminator and generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26CrlGquQSlp"
      },
      "source": [
        "# define the standalone discriminator model\n",
        "def define_discriminator(in_shape=(32,32,3)):\n",
        "    model = Sequential()\n",
        "    # normal\n",
        "    model.add(Conv2D(64, (3,3), padding='same', input_shape=in_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    # downsample\n",
        "    model.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    # downsample\n",
        "    model.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    # downsample\n",
        "    model.add(Conv2D(256, (3,3), strides=(2,2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    # classifier\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    # compile model\n",
        "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "# define the standalone generator model\n",
        "def define_generator(latent_dim):\n",
        "    model = Sequential()\n",
        "    # foundation for 4x4 image\n",
        "    n_nodes = 256 * 8 * 8\n",
        "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Reshape((8, 8, 256)))\n",
        "    # # upsample to 8x8\n",
        "    # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "    # model.add(LeakyReLU(alpha=0.2))\n",
        "    # upsample to 16x16\n",
        "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    # upsample to 32x32\n",
        "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    # output layer\n",
        "    model.add(Conv2D(3, (3,3), activation='sigmoid', padding='same'))\n",
        "    return model\n",
        "\n",
        "\n",
        "# define the combined generator and discriminator model, for updating the generator\n",
        "def define_gan(g_model, d_model):\n",
        "    # make weights in the discriminator not trainable\n",
        "    d_model.trainable = False\n",
        "    # connect them\n",
        "    model = Sequential()\n",
        "    # add generator\n",
        "    model.add(g_model)\n",
        "    # add the discriminator\n",
        "    model.add(d_model)\n",
        "    # compile model\n",
        "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "    return model\n",
        "\n",
        "\n",
        "# select real samples\n",
        "def generate_real_samples(dataset, n_samples):\n",
        "    # choose random instances\n",
        "    ix = randint(0, dataset.shape[0], n_samples)\n",
        "    # retrieve selected images\n",
        "    X = dataset[ix]\n",
        "    # generate 'real' class labels (1)\n",
        "    y = ones((n_samples, 1))\n",
        "    return X, y\n",
        "\n",
        "\n",
        "# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "    # generate points in the latent space\n",
        "    x_input = randn(latent_dim * n_samples)\n",
        "    # reshape into a batch of inputs for the network\n",
        "    x_input = x_input.reshape(n_samples, latent_dim)\n",
        "    return x_input\n",
        "\n",
        "\n",
        "# use the generator to generate n fake examples, with class labels\n",
        "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
        "    # generate points in latent space\n",
        "    x_input = generate_latent_points(latent_dim, n_samples)\n",
        "    # predict outputs\n",
        "    X = g_model.predict(x_input)\n",
        "    # create 'fake' class labels (0)\n",
        "    y = zeros((n_samples, 1))\n",
        "    return X, y\n",
        "\n",
        "\n",
        "# use the generator to generate n fake examples for GIF\n",
        "def generate_gif_images(g_model, n_samples):\n",
        "    # predict outputs\n",
        "    X = g_model.predict(seed)\n",
        "    # create 'fake' class labels (0)\n",
        "    y = zeros((n_samples, 1))\n",
        "    return X, y\n",
        "\n",
        "\n",
        "# create and save a plot of generated images\n",
        "def save_plot(examples, epoch, n=3):\n",
        "    # # Reshape from [-1,1] to [0,1]\n",
        "    # examples = (examples + 1) / 2\n",
        "\n",
        "    # plot images\n",
        "    pyplot.figure(figsize=(8,8))\n",
        "    for i in range(n * n):\n",
        "        # define subplot\n",
        "        pyplot.subplot(n, n, 1 + i)\n",
        "        # turn off axis\n",
        "        pyplot.axis('off')\n",
        "        # plot raw pixel data\n",
        "        pyplot.imshow(examples[i, :, :, :])\n",
        "\n",
        "    # save plot to file\n",
        "    filename = 'Generated images/generated_plot_e%03d.png' % (epoch+1)\n",
        "    pyplot.savefig(filename)\n",
        "\n",
        "    pyplot.show()\n",
        "\n",
        "\n",
        "# evaluate the discriminator, plot generated images, save generator model\n",
        "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples):\n",
        "    # prepare real samples\n",
        "    X_real, y_real = generate_real_samples(dataset, n_samples)\n",
        "    # evaluate discriminator on real examples\n",
        "    _, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
        "    # prepare fake examples\n",
        "    x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
        "    # evaluate discriminator on fake examples\n",
        "    _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
        "    # summarize discriminator performance\n",
        "    print('> epoch:', (epoch+1) ,' Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
        "    # save plot\n",
        "    x_gif_images, y_gif_images = generate_gif_images(g_model, 9)\n",
        "    save_plot(x_gif_images, epoch)\n",
        "    # save the generator model tile file\n",
        "    filename = 'Trained models/generator_model_%03d.h5' % (epoch+1)\n",
        "    g_model.save(filename)\n",
        "\n",
        "\n",
        "# train the generator and discriminator\n",
        "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=200, n_batch=128):\n",
        "    bat_per_epo = int(dataset.shape[0] / n_batch)\n",
        "    half_batch = int(n_batch / 2.0)\n",
        "    # manually enumerate epochs\n",
        "    for i in range(n_epochs):\n",
        "        # enumerate batches over the training set\n",
        "        for j in range(bat_per_epo):\n",
        "            # get randomly selected 'real' samples\n",
        "            X_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "            # update discriminator model weights\n",
        "            d_loss1, _ = d_model.train_on_batch(X_real, y_real)\n",
        "            # generate 'fake' examples\n",
        "            X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "            # update discriminator model weights\n",
        "            d_loss2, _ = d_model.train_on_batch(X_fake, y_fake)\n",
        "            # prepare points in latent space as input for the generator\n",
        "            X_gan = generate_latent_points(latent_dim, n_batch)\n",
        "            # create inverted labels for the fake samples\n",
        "            y_gan = ones((n_batch, 1))\n",
        "            # update the generator via the discriminator's error\n",
        "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
        "\n",
        "            # Log training stats\n",
        "            with summary_writer.as_default():\n",
        "                tf.summary.scalar('gen_loss', g_loss, step=i)\n",
        "                tf.summary.scalar('disc_loss_real', d_loss1, step=i)\n",
        "                tf.summary.scalar('disc_loss_fake', d_loss2, step=i)\n",
        "\n",
        "        # print model performance, sometimes\n",
        "        if (i+1) % 1 == 0:\n",
        "            display.clear_output(wait=True)\n",
        "            summarize_performance(i, g_model, d_model, dataset, latent_dim, 9)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w56Ay5lhzGo5"
      },
      "source": [
        "# Train models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxr3rwb0Wp5v"
      },
      "source": [
        "# We will reuse this seed overtime (so it's easier)\n",
        "# to visualize progress in the animated GIF)\n",
        "\n",
        "n_samples = 9\n",
        "latent_dim = 100\n",
        "\n",
        "# generate points in the latent space\n",
        "seed = randn(latent_dim * n_samples)\n",
        "# reshape into a batch of inputs for the network\n",
        "seed = seed.reshape(n_samples, latent_dim)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dA34dHbQb2U"
      },
      "source": [
        "EPOCHS = 200\n",
        "# size of the latent space\n",
        "latent_dim = 100\n",
        "# create the discriminator\n",
        "from keras.models import load_model\n",
        "d_model = define_discriminator()\n",
        "# create the generator\n",
        "g_model = define_generator(latent_dim)\n",
        "# g_model = load_model('generator_model_200.h5')\n",
        "# create the gan\n",
        "gan_model = define_gan(g_model, d_model)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzDGlEC7b5Zq"
      },
      "source": [
        "# Log training stats\n",
        "import datetime\n",
        "log_dir = \"logs/\"\n",
        "summary_writer = tf.summary.create_file_writer(\n",
        "log_dir + datetime.datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\"))\n",
        "\n",
        "# Create new output folders\n",
        "os.makedirs('Generated images')\n",
        "os.makedirs('Trained models')\n",
        "\n",
        "# Open Tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2_Gs7a4b0Lz"
      },
      "source": [
        "# train model\n",
        "train(g_model, d_model, gan_model, train_images, latent_dim, EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYlWzqLwkaV5"
      },
      "source": [
        "## Create a GIF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UO1Y7CMn5W3"
      },
      "source": [
        "# # To generate GIFs\n",
        "# !pip install imageio\n",
        "# !pip install git+https://github.com/tensorflow/docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SzQAHemkTlh"
      },
      "source": [
        "# Display a single image using the epoch number\n",
        "def display_image(epoch):\n",
        "  return PIL.Image.open('Generated images/generated_plot_e%03d.png' % (epoch-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6ZdS8sckdzJ"
      },
      "source": [
        "display_image(60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdt1995gkgLA"
      },
      "source": [
        "anim_file = 'dcgan.gif'\n",
        "\n",
        "with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "  filenames = glob.glob('Generated images/generated_plot*.png')\n",
        "  filenames = sorted(filenames)\n",
        "  for index, filename in enumerate(filenames):\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "    if index == EPOCHS:\n",
        "        break\n",
        "  image = imageio.imread(filename)\n",
        "  writer.append_data(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVr5B6y5kiTb"
      },
      "source": [
        "import tensorflow_docs.vis.embed as embed\n",
        "embed.embed_file(anim_file)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
